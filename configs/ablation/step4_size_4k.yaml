# Step 4. Training

base:
  save_path: /nas2/data/Janus_dataset/next/ckpt/ablation
  exp_name: 1106_argmax_chosen_prompt_size_4k
  world_size: 4
  resume: 

model:
  model_path: /nas2/checkpoints/Janus-Pro-7B
  cache_dir: /nas2/checkpoints/hf_cache_yj
  attn_mode: flash_attention_2 # eager

# 참고
# LLAMA_ATTENTION_CLASSES = {
#     "eager": LlamaAttention,
#     "flash_attention_2": LlamaFlashAttention2,
#     "sdpa": LlamaSdpaAttention,
# }

use_mask: True
mask_dir: /nas2/data/Janus_dataset/next_v2/object_mask/argmax_chosen_10430

use_peft: True 
lora:
  lora_rank: 64
  lora_alpha: 128
  lora_dropout: 0.05
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "down_proj", "up_proj"]
  modules_to_save: # Keeps selected layers trainable without applying LoRA.


dataset:
  train:
    batch_size: 16 # 8 # [WARNING] When train in copo_mode=True, use less batch size than when the copo_mode=False 
    num_workers: 4
    data_path: /nas2/data/Janus_dataset/next_v2/ablation/init_prompt_size/init_prompt_size_4.json
    num_samples:              
  val:

optimizer:
  init_lr: 1e-5
  betas: [0.9, 0.95]
  weight_decay: 0.00 
  eps: 1e-8 
  scheduler_type: cosine # constant # cosine
  min_lr: 1e-7

# 166 step (4k; 8 epoch)
experiment:
  seed: 42
  precision: bf16 # 32 / bf16
  warmup_ratio: 0.05 # 0.15
  gradient_clip_val: 1.0
  enable_checkpointing: True           # checkpoint saving 관련
  gradient_checkpointing: True         # model training 관련
  gradient_accumulation_steps: 2 # 4       # (GC on)
  max_training_steps: 240 # 1000           # 8 epoch, 기존에는 80 step = 1 epoch (16k 기준)
  save_steps: 80
  log_steps: 5
  val_steps: 5
  log_by_category: True


  freeze:
    vision_model: True
    aligner: True
    gen_vision_model: True
    gen_aligner: True
    gen_head: True
    gen_embed: True
    language_model: False

tokenizer:
  max_length: 2048
  max_prompt_length: 1024
  truncation_mode: keep_end 
  truncation_side: left 
  label_pad_token_id: -100  

algo: 
  beta_1: 5 # 2 # 5
  beta_2: 5 # 5 # 2 # 5 # copo
  gamma_beta_ratio_1: 0.5 
  gamma_beta_ratio_2: 0.05

  label_smoothing: 0.0 
  loss_type: sigmoid

  # loss weight 
  schedule_loss_weight: False # schedule_loss_weight 내에 min, max 값 하드코딩

  simpo_weight: 1.0 # 0.8 # 1.0
  copo_weight: 0.0 # 1.0 # 0.8  # 1.0 # copo_weight > 0.0, automatically set mdpo_mode=True 
  sft_weight: 2.0 # 1.0

  pixel_weight: 0.0 # 1.2 # 1.0
  pixel_tau: 0.0 # 1.0 # 1.0 ~ 0.0 (cannot be 0.0)
  pixel_type: # mse # l1

  mask_alpha: 1.0 # 1.0
  mask_gamma: 1.0

  # reward clipping weight
  simpo_clipping_weight: 0.0
  copo_clipping_weight: 0.0

